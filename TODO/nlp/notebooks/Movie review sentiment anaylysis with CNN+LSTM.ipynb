{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2c76dab606870aa66b8a9da2cad9589413af3d0"
   },
   "source": [
    "# 介绍 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f999aedc86bd6936feae9cf9298fc059cd80e01"
   },
   "source": [
    "NLP情感分析学习. 参考资料[kaggle比赛](https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only),[kaggle kernel: Movie Review Sentiment Analysis EDA and models](https://www.kaggle.com/artgor/movie-review-sentiment-analysis-eda-and-models), [kernel2](https://www.kaggle.com/prithiviraj/beginner-friendly-fastext-bilstmcnn-top-1/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "474f5527e266016f7ea6920698cc85b8f4583aec"
   },
   "source": [
    "# Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ee652abb6bc2f69a61d388a2339c5efb45adb1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b99f99486e6a32cb8f935a1f0980bce230d74896"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e93ac9418a5d5d984d93c7aa9296617d756b7ee2"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bb07a7e0f6bf6976b307e5c2f0ef96f069c5424"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5dcfb0ec2aabf09f60fadb036be8d9b8987185b0"
   },
   "outputs": [],
   "source": [
    "help(np.percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c018894738475dc8615e32f5a7d2d70fd4ef85ba"
   },
   "outputs": [],
   "source": [
    "def df_info(df):\n",
    "    print('='*20)\n",
    "    print('# of row in df: {}'.format(len(df)))\n",
    "    print('# of PhraseId: {0}'.format(df.groupby('PhraseId').size().sum()))\n",
    "    print('avg # of SentencId in every PhraseId :{0:.0f}'.format(df.groupby('SentenceId').size().mean()))\n",
    "#     print('avg # of word in each sentence is: {0:.0f}'.format(df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).mean()))\n",
    "    values = df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).values\n",
    "#     print(len(values))\n",
    "#     print(np.percentile(values,[0, 25, 50, 75, 100]))\n",
    "    print('stat # of word in each sentence is: {}'.format(\n",
    "        np.percentile(\n",
    "            values, \n",
    "            [0, 25, 50, 75, 100]\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41641e27b0051cd60426dd5f79e958bad80e566e"
   },
   "outputs": [],
   "source": [
    "df_info(train)\n",
    "df_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfce2f4a2df6c3c6bcce887567f498df04ff5477"
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "values = df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3495f185555f84c7f5b25977111191108ba6df19"
   },
   "outputs": [],
   "source": [
    "np.percentile(values, [0, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bcaf71749144128f530ec05672a391c0a5f00f53"
   },
   "source": [
    "# 传统机器学习过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b50ccfb59404f1f5be1ca7f5ff63fe6bcdc80508"
   },
   "source": [
    "## Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81a4fd1fb0cf5eb6fd28af354f07419f9f594917",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aca9fcb9aeefb2e6956e5a1a7453e3ff96d0e8f"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=[1, 2], tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d11993fad0ab0e5403bb81ec8b482f237c6eef8"
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05dcf8702bcf03df83c363fda8b2159dce3f3ae8"
   },
   "outputs": [],
   "source": [
    "train_x = vectorizer.transform(train['Phrase'])\n",
    "test_x = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c132da657bf3436cd88f52380a5f6b290252cbed"
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cccfe51f6da4b91e40356f4a1ea0164054e9f658"
   },
   "outputs": [],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c8666e73700fc0d6a0eeb2eeb678bdd4f639b93"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2613815e03267aa9e82050dcf8c7be111ef67c2e"
   },
   "source": [
    "## Train & evalationa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a979d131f67f8e548bd3ec1e3cd9114ea5394af9"
   },
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e0c4f83c2006ea8b9d13429f80225866b7902e7"
   },
   "outputs": [],
   "source": [
    "log = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b6f29a17348310150b2fb7a5dad3ddbe91352e5"
   },
   "outputs": [],
   "source": [
    "score = cross_val_score(ovr, X=train_x, y = y, cv=3).mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a139747019f643cce2e2c41683b073707486383"
   },
   "outputs": [],
   "source": [
    "svm = LinearSVC(\n",
    "#     dual='dual'\n",
    ")\n",
    "score = cross_val_score(svm, X=train_x, y = y, cv=3).mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "675399c6c8e9fc9b8e6b2675735c0e91c395b889"
   },
   "source": [
    "# 深度学习 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a1d86c5f499d119b9e7fb48c279571c513e82d5"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, Flatten, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAvgPool1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6687ae42ff67d1f86d68e5e5ad0ca021fb550d9d"
   },
   "source": [
    "## Tokenlized and embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2c3b87fea131fd512acadbe7ad92ef36489e572"
   },
   "source": [
    "### Tokenlized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f03cae1ba92afea627671124f3d7243595259fe"
   },
   "outputs": [],
   "source": [
    "# tk = Tokenizer(lower=True, filtesr=' ')\n",
    "tk = Tokenizer(lower=True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3928c8e073f2c95171c7449c0d9fe26c5008f273"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9526d5a80fcf467834b2e02e26f1045d941ad276"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "250b863b1ce06900361e27cef189127ca3fb31d5"
   },
   "source": [
    "###  pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47b70c5d08bd3d5a48f4d6e29a78892b5fc081f4"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "train_X = pad_sequences(maxlen=max_len, sequences=train_tokenized)\n",
    "test_X = pad_sequences(maxlen=max_len, sequences=test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "635d3ffd76edd48a1859ec107b68c39e830ae40b"
   },
   "outputs": [],
   "source": [
    "# ?????\n",
    "\n",
    "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "embed_size = 300\n",
    "max_features = 30000\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73c3347597762ecbcacc374f837e224cfde4c213"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49ecf314ba65a1eb5daf1026bbd87258a8df6b5c"
   },
   "outputs": [],
   "source": [
    "np.shape(y_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52bbe028e76c15fc2ce7f9b56eae45c364e84534"
   },
   "source": [
    "## Model defination and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da5a3eccdb2e605c0747edc03cfe7d32090435a0"
   },
   "outputs": [],
   "source": [
    "def build_model1(units, spatial_dr=1, cov_size=32, kernel_size1=3, kernel_size2=2, dense_unit=128, dr=0.1, lr=0.001, lr_d=0.0):\n",
    "    file_path = 'model.hdf5'\n",
    "    check_point = ModelCheckpoint(\n",
    "        filepath=file_path,\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    earlystopping = EarlyStopping(\n",
    "#         monitor = 'binary_crossentropy'\n",
    "        monitor = 'val_loss',\n",
    "        patience = 3,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    \n",
    "    # Embedding layer \n",
    "#     ????????  shape ??\n",
    "#     inp = Input(shape=(embed_size, ))\n",
    "    inp = Input(shape=(max_len, ))\n",
    "    x = Embedding(\n",
    "        input_dim = 19479,\n",
    "        output_dim = embed_size, \n",
    "        weights = [embedding_matrix],\n",
    "        trainable = False\n",
    "    )(inp)\n",
    "    x = SpatialDropout1D(spatial_dr)(x)\n",
    "    \n",
    "    # return_sequences=True, 输出cell_state 和hiden_state\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences=True))(x)\n",
    "#     print(x_gru.shape)\n",
    "    x1 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size1,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'\n",
    "    )(x_gru)\n",
    "    maxpool_x1 = GlobalMaxPooling1D()(x1)\n",
    "    avgpool_x1 = GlobalAvgPool1D()(x1)\n",
    "    \n",
    "    x2 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size2,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'        \n",
    "    )(x_gru)\n",
    "    maxpool_x2 = GlobalMaxPooling1D()(x2)\n",
    "    avgpool_x2 = GlobalAvgPool1D()(x2)    \n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n",
    "    x3 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size1,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'\n",
    "    )(x_lstm)\n",
    "    maxpool_x3 = GlobalMaxPooling1D()(x3)\n",
    "    avgpool_x3 = GlobalAvgPool1D()(x3)   \n",
    "    \n",
    "    x4 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size2,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'\n",
    "    )(x_lstm)\n",
    "    maxpool_x4 = GlobalMaxPooling1D()(x4)\n",
    "    avgpool_x4 = GlobalAvgPool1D()(x4)\n",
    "    \n",
    "    x = concatenate([\n",
    "                     maxpool_x1, avgpool_x1, \n",
    "                     maxpool_x2, avgpool_x2,\n",
    "                     maxpool_x3, avgpool_x3,\n",
    "                     maxpool_x4, avgpool_x4\n",
    "                    ])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_unit, activation='relu')(x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_unit/2), activation='relu')(x))\n",
    "#     output = Dense(5, activation='softmax')(x)\n",
    "    output = Dense(5, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(input=inp, output=output)\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=lr, decay=lr_d),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    model.fit(\n",
    "#         x = embedding_matrix,\n",
    "        x = train_X,\n",
    "#         y = y,\n",
    "        y= y_ohe,\n",
    "        batch_size = 128,\n",
    "        epochs = 20,\n",
    "        verbose = 1,\n",
    "        callbacks = [check_point, earlystopping],\n",
    "        validation_split = 0.1\n",
    "    )\n",
    "    model.load_model(file_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "070d3ec511ff38e611c394992d794efc1951eeb9"
   },
   "outputs": [],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_unit=32, dr=0.1, cov_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d23e47517cb6e10c1a35e6e198d817e7a7ef6702"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1"
   },
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n",
    "\n",
    "This dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dea95068fd46eef3b78ee878c86c8f037c8d42a"
   },
   "source": [
    "# 探索 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c"
   },
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e94f8371d8be87186f23ecc75178480d3d96bd78"
   },
   "outputs": [],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f75e24b86e3aeb7477fa1cd69789992233420b1"
   },
   "outputs": [],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e"
   },
   "outputs": [],
   "source": [
    "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
    "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bef53d8f659b78e3fc6b1ed07025591d55b7c128"
   },
   "outputs": [],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e26b87ca71285ab9ee43a399d0534cbc03843df5"
   },
   "outputs": [],
   "source": [
    "train['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2"
   },
   "source": [
    "`We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. **Sometimes one word or even one punctuation mark influences the sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d1efbed65f250d37472544f4fe37cb6fd13e183",
    "collapsed": true
   },
   "source": [
    "Let's see for example most common trigrams for positive phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dea3ff8165291f82b0516dc7bf13691d653e2d31"
   },
   "source": [
    "ngrams:  几个为一组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a128ab1d36eef6ec47161705a2b1094b830fe10"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccc69cbbe06084a848a35d47644d936334cbb479",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_trigrams[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5c8533c5861794d44b53dbb2e5ef764e5e88119"
   },
   "outputs": [],
   "source": [
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "402c0b58fe43a680bea33e1813012bec5c16cb55"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f59db6ed32fafb024728fd95b96157f278682a74"
   },
   "source": [
    "The results show the main problem with this dataset: **there are to many common words due to sentenced splitted in phrases**. As a result stopwords shouldn't be removed from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10dc8fc8d535ef492a3dab1b85b4052feec756ee"
   },
   "source": [
    "### Thoughts on feature processing and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b"
   },
   "source": [
    "So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n",
    "- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n",
    "- puntuation could be important, so it should be used;\n",
    "- ngrams are necessary to get the most info from data;\n",
    "- using features like word count or sentence length won't be useful;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72cdd2c86a3996ab7ca41971046dfff41d6a9d42"
   },
   "source": [
    "# 编码  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50c49caf376cd22a5af930c335813b3131e8bf4a"
   },
   "source": [
    "?? ngram_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "828ed5a9ee5143b5542eccc6d90daa3e9ddc9edd"
   },
   "source": [
    "编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ebc937fd5ec811bc5c529ff416180fe338d073d"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d0bb4539b1e0b5f8439878a967ce7b5ece23f60"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42d31643e53c1d2eb5f74d339bd494e5b49c3a25",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vectorized[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3a214237f75ef8ba2887d61f62c215459a3484f"
   },
   "outputs": [],
   "source": [
    "help(OneVsRestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb023b286a7f05f06c55c8b2bcbb46ccbe7c041b"
   },
   "source": [
    "# 训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75a7acfd815fb391cad92eed8d2f13e5c9801de8"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a0bf05b880e0d05da378b753394e5a631753e82"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "454e2a9316ba267ce66b1abf7da89da315351290"
   },
   "outputs": [],
   "source": [
    "help(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe"
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "755c2b04f7bf8e86e9dc6242f392a575c61a052c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ccaedd2a27eaa8d6859b2b66bd4d3ad3d0587e4"
   },
   "source": [
    "# 评估 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55a5845de3412244cfa9f0a11392e50c76d78184"
   },
   "outputs": [],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7caffe8462d1c37b4e0b0bbe60190cde7231328"
   },
   "source": [
    "1.keras.layers:\n",
    "    1.  Bidirectional rnn: 双向rnn\n",
    "    2. GRU:  Gated Recurrent Unit, LSTM 变体, 加入忘记门和输入门 \n",
    "    3.  GlobalMaxPool1D, MaxPooling1D [差别](https://stackoverflow.com/questions/43728235/what-is-the-difference-between-keras-maxpooling1d-and-globalmaxpooling1d-functi)\n",
    "    4. constrains:  functions that impose constraints on weight values(e.g MaxNorm, MinMaxNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8f66a8817fe7c3ec8b961551ee68a12e16d0f86"
   },
   "source": [
    "# 编码 tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c9acc6a28970c178ad63ff347b7a118a7b7b13f"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "dir(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80"
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1724669c7ca010d1bbf75e200211afdead768d1f"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2891fd68d0e7cfe49ad882595b431856f8eee8a7"
   },
   "outputs": [],
   "source": [
    "print(len(train_tokenized))\n",
    "print(len(train_tokenized[0]))\n",
    "print(len(train_tokenized[20]))\n",
    "\n",
    "\n",
    "# print(test_tokenized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af02306e353ea2ef4a8b537f75f4ee8294118d18"
   },
   "source": [
    "# pad ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcb4482d43f171509a5854b09dd6a8590a8ed1e0"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "399d2b1ee867e3223cdd078e1a7c929597fcc734",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b1f4574457545c8d268c8e52c044c154a053338"
   },
   "source": [
    "# Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d74c2aa2f13e8544f045e5644d5bac70e248a8bd"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdb522c23b75331481145b789cf127b39d47eaa1"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "539088c6a01cc12046298b7371628f5cc03ef613"
   },
   "outputs": [],
   "source": [
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63bfe54e55dc24a6734970da5ea7ab9b46bc8b0e"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "617dc05409aacfd8739765111f72606db434af91"
   },
   "source": [
    "# build model and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51844fcec05cd42f6de6751e2342ac8936909114",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(CuDNNGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "594273c2d887315d083a35a5ffd7c2dd40c2ebb6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    # callback function; save the model after every epoch\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    # 直接load训练好的为embedding_matrix 没在这儿训练\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    # 丢掉整个dim, 普通的丢掉几个元素而已\n",
    "    # ? 下一步如何衔接dim?\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "    \n",
    "    # bidirection wrapper in rnn\n",
    "    # CuDNN... 英伟达出品,只能在GPU上跑\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    # global average pooling operation for temporal data\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    # \n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f52b1f6de4e939c8d21e3525503912282fbd47"
   },
   "source": [
    "An attempt at ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5eb586c98fb75c25cac099cd03d8233185fdc317"
   },
   "outputs": [],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b059392aad7d904adfb8ae151ad2004aa03da30d"
   },
   "outputs": [],
   "source": [
    "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe"
   },
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bf90d6d4effebb3c5aa9b666b8e09c9c57d94d3"
   },
   "outputs": [],
   "source": [
    "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf6d8d367c5adc30e00bbd77c1de70fd52960441"
   },
   "outputs": [],
   "source": [
    "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b10113439be683bf19930750eaf96328e5b58d42"
   },
   "outputs": [],
   "source": [
    "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8529014d1a239f308ac5f2552088ce2d8ebb8966"
   },
   "outputs": [],
   "source": [
    "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred1\n",
    "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred2\n",
    "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred3\n",
    "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d58a5b52ea647dab51123ef89878c5355b3d2971"
   },
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['Sentiment'] = predictions\n",
    "sub.to_csv(\"blend.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
