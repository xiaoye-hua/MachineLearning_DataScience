{
  "cells": [
    {
      "metadata": {
        "_uuid": "c2c76dab606870aa66b8a9da2cad9589413af3d0"
      },
      "cell_type": "markdown",
      "source": "# 介绍 "
    },
    {
      "metadata": {
        "_uuid": "6f999aedc86bd6936feae9cf9298fc059cd80e01"
      },
      "cell_type": "markdown",
      "source": "NLP情感分析学习. 参考资料[kaggle kernel: Movie Review Sentiment Analysis EDA and models](https://www.kaggle.com/artgor/movie-review-sentiment-analysis-eda-and-models)"
    },
    {
      "metadata": {
        "_uuid": "474f5527e266016f7ea6920698cc85b8f4583aec"
      },
      "cell_type": "markdown",
      "source": "# Modules "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee652abb6bc2f69a61d388a2339c5efb45adb1b"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk import TweetTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import cross_val_score\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b99f99486e6a32cb8f935a1f0980bce230d74896"
      },
      "cell_type": "markdown",
      "source": "# EDA"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e93ac9418a5d5d984d93c7aa9296617d756b7ee2"
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bb07a7e0f6bf6976b307e5c2f0ef96f069c5424"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5dcfb0ec2aabf09f60fadb036be8d9b8987185b0"
      },
      "cell_type": "code",
      "source": "help(np.percentile)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c018894738475dc8615e32f5a7d2d70fd4ef85ba"
      },
      "cell_type": "code",
      "source": "def df_info(df):\n    print('='*20)\n    print('# of row in df: {}'.format(len(df)))\n    print('# of PhraseId: {0}'.format(df.groupby('PhraseId').size().sum()))\n    print('avg # of SentencId in every PhraseId :{0:.0f}'.format(df.groupby('SentenceId').size().mean()))\n#     print('avg # of word in each sentence is: {0:.0f}'.format(df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).mean()))\n    values = df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).values\n#     print(len(values))\n#     print(np.percentile(values,[0, 25, 50, 75, 100]))\n    print('stat # of word in each sentence is: {}'.format(\n        np.percentile(\n            values, \n            [0, 25, 50, 75, 100]\n        )\n    )\n    )\n    \n    print('='*20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41641e27b0051cd60426dd5f79e958bad80e566e"
      },
      "cell_type": "code",
      "source": "df_info(train)\ndf_info(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfce2f4a2df6c3c6bcce887567f498df04ff5477"
      },
      "cell_type": "code",
      "source": "df = train\nvalues = df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3495f185555f84c7f5b25977111191108ba6df19"
      },
      "cell_type": "code",
      "source": "np.percentile(values, [0, 50])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcaf71749144128f530ec05672a391c0a5f00f53"
      },
      "cell_type": "markdown",
      "source": "# 传统机器学习过程 "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b50ccfb59404f1f5be1ca7f5ff63fe6bcdc80508"
      },
      "cell_type": "markdown",
      "source": "## Feature extraction "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "81a4fd1fb0cf5eb6fd28af354f07419f9f594917"
      },
      "cell_type": "code",
      "source": "tokenizer = TweetTokenizer()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1aca9fcb9aeefb2e6956e5a1a7453e3ff96d0e8f"
      },
      "cell_type": "code",
      "source": "vectorizer = TfidfVectorizer(ngram_range=[1, 2], tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d11993fad0ab0e5403bb81ec8b482f237c6eef8"
      },
      "cell_type": "code",
      "source": "vectorizer.fit(full_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05dcf8702bcf03df83c363fda8b2159dce3f3ae8"
      },
      "cell_type": "code",
      "source": "train_x = vectorizer.transform(train['Phrase'])\ntest_x = vectorizer.transform(test['Phrase'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c132da657bf3436cd88f52380a5f6b290252cbed"
      },
      "cell_type": "code",
      "source": "train_x.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cccfe51f6da4b91e40356f4a1ea0164054e9f658"
      },
      "cell_type": "code",
      "source": "test_x.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c8666e73700fc0d6a0eeb2eeb678bdd4f639b93"
      },
      "cell_type": "code",
      "source": "y = train['Sentiment']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2613815e03267aa9e82050dcf8c7be111ef67c2e"
      },
      "cell_type": "markdown",
      "source": "## Train & evalationa "
    },
    {
      "metadata": {
        "_uuid": "a979d131f67f8e548bd3ec1e3cd9114ea5394af9"
      },
      "cell_type": "markdown",
      "source": "### Logistic regression "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e0c4f83c2006ea8b9d13429f80225866b7902e7"
      },
      "cell_type": "code",
      "source": "log = LogisticRegression()\novr = OneVsRestClassifier(log)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b6f29a17348310150b2fb7a5dad3ddbe91352e5"
      },
      "cell_type": "code",
      "source": "score = cross_val_score(ovr, X=train_x, y = y, cv=3).mean()\nprint(score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a139747019f643cce2e2c41683b073707486383"
      },
      "cell_type": "code",
      "source": "svm = LinearSVC(\n#     dual='dual'\n)\nscore = cross_val_score(svm, X=train_x, y = y, cv=3).mean()\nprint(score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "675399c6c8e9fc9b8e6b2675735c0e91c395b889"
      },
      "cell_type": "markdown",
      "source": "# 深度学习 "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a1d86c5f499d119b9e7fb48c279571c513e82d5"
      },
      "cell_type": "code",
      "source": "from keras.layers import Input, Dense, Dropout, Flatten, SpatialDropout1D, BatchNormalization\nfrom keras.layers import GlobalMaxPooling1D, GlobalAvgPool1D\nfrom keras.layers import Embedding\nfrom keras.layers import CuDNNGRU, CuDNNLSTM\nfrom keras.layers import Bidirectional\nfrom keras.layers import Conv1D\nfrom keras.layers import concatenate\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\nfrom keras.models import Model, load_model\nfrom keras.preprocessing.text import Tokenizer, one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import OneHotEncoder",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6687ae42ff67d1f86d68e5e5ad0ca021fb550d9d"
      },
      "cell_type": "markdown",
      "source": "## Tokenlized and embedding "
    },
    {
      "metadata": {
        "_uuid": "e2c3b87fea131fd512acadbe7ad92ef36489e572"
      },
      "cell_type": "markdown",
      "source": "### Tokenlized "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f03cae1ba92afea627671124f3d7243595259fe"
      },
      "cell_type": "code",
      "source": "# tk = Tokenizer(lower=True, filtesr=' ')\ntk = Tokenizer(lower=True, filters='')\ntk.fit_on_texts(full_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3928c8e073f2c95171c7449c0d9fe26c5008f273"
      },
      "cell_type": "code",
      "source": "train.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9526d5a80fcf467834b2e02e26f1045d941ad276"
      },
      "cell_type": "code",
      "source": "train_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "250b863b1ce06900361e27cef189127ca3fb31d5"
      },
      "cell_type": "markdown",
      "source": "###  pad"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47b70c5d08bd3d5a48f4d6e29a78892b5fc081f4"
      },
      "cell_type": "code",
      "source": "max_len = 50\ntrain_X = pad_sequences(maxlen=max_len, sequences=train_tokenized)\ntest_X = pad_sequences(maxlen=max_len, sequences=test_tokenized)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "635d3ffd76edd48a1859ec107b68c39e830ae40b"
      },
      "cell_type": "code",
      "source": "# ?????\n\nembedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nembed_size = 300\nmax_features = 30000\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73c3347597762ecbcacc374f837e224cfde4c213"
      },
      "cell_type": "code",
      "source": "ohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49ecf314ba65a1eb5daf1026bbd87258a8df6b5c"
      },
      "cell_type": "code",
      "source": "np.shape(y_ohe)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52bbe028e76c15fc2ce7f9b56eae45c364e84534"
      },
      "cell_type": "markdown",
      "source": "## Model defination and train "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da5a3eccdb2e605c0747edc03cfe7d32090435a0"
      },
      "cell_type": "code",
      "source": "def build_model1(units, spatial_dr=1, cov_size=32, kernel_size1=3, kernel_size2=2, dense_unit=128, dr=0.1, lr=0.001, lr_d=0.0):\n    file_path = 'model.hdf5'\n    check_point = ModelCheckpoint(\n        filepath=file_path,\n        verbose = 1,\n        save_best_only = True,\n        mode = 'min'\n    )\n    earlystopping = EarlyStopping(\n#         monitor = 'binary_crossentropy'\n        monitor = 'val_loss',\n        patience = 3,\n        mode = 'min'\n    )\n    \n    # Embedding layer \n#     ????????  shape ??\n#     inp = Input(shape=(embed_size, ))\n    inp = Input(shape=(max_len, ))\n    x = Embedding(\n        input_dim = 19479,\n        output_dim = embed_size, \n        weights = [embedding_matrix],\n        trainable = False\n    )(inp)\n    x = SpatialDropout1D(spatial_dr)(x)\n    \n    # return_sequences=True, 输出cell_state 和hiden_state\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences=True))(x)\n#     print(x_gru.shape)\n    x1 = Conv1D(\n        filters = cov_size,\n        kernel_size=kernel_size1,\n        padding = 'valid',\n        kernel_initializer = 'he_uniform'\n    )(x_gru)\n    maxpool_x1 = GlobalMaxPooling1D()(x1)\n    avgpool_x1 = GlobalAvgPool1D()(x1)\n    \n    x2 = Conv1D(\n        filters = cov_size,\n        kernel_size=kernel_size2,\n        padding = 'valid',\n        kernel_initializer = 'he_uniform'        \n    )(x_gru)\n    maxpool_x2 = GlobalMaxPooling1D()(x2)\n    avgpool_x2 = GlobalAvgPool1D()(x2)    \n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n    x3 = Conv1D(\n        filters = cov_size,\n        kernel_size=kernel_size1,\n        padding = 'valid',\n        kernel_initializer = 'he_uniform'\n    )(x_lstm)\n    maxpool_x3 = GlobalMaxPooling1D()(x3)\n    avgpool_x3 = GlobalAvgPool1D()(x3)   \n    \n    x4 = Conv1D(\n        filters = cov_size,\n        kernel_size=kernel_size2,\n        padding = 'valid',\n        kernel_initializer = 'he_uniform'\n    )(x_lstm)\n    maxpool_x4 = GlobalMaxPooling1D()(x4)\n    avgpool_x4 = GlobalAvgPool1D()(x4)\n    \n    x = concatenate([\n                     maxpool_x1, avgpool_x1, \n                     maxpool_x2, avgpool_x2,\n                     maxpool_x3, avgpool_x3,\n                     maxpool_x4, avgpool_x4\n                    ])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_unit, activation='relu')(x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_unit/2), activation='relu')(x))\n#     output = Dense(5, activation='softmax')(x)\n    output = Dense(5, activation='sigmoid')(x)\n    \n    model = Model(input=inp, output=output)\n    model.compile(\n        optimizer = Adam(lr=lr, decay=lr_d),\n        loss='binary_crossentropy', \n        metrics=['accuracy'],\n    )\n    model.fit(\n#         x = embedding_matrix,\n        x = train_X,\n#         y = y,\n        y= y_ohe,\n        batch_size = 128,\n        epochs = 20,\n        verbose = 1,\n        callbacks = [check_point, earlystopping],\n        validation_split = 0.1\n    )\n    model.load_model(file_path)\n    return model\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "070d3ec511ff38e611c394992d794efc1951eeb9"
      },
      "cell_type": "code",
      "source": "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_unit=32, dr=0.1, cov_size=32)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d23e47517cb6e10c1a35e6e198d817e7a7ef6702"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1"
      },
      "cell_type": "markdown",
      "source": "## General information\n\nIn this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n\nThis dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6dea95068fd46eef3b78ee878c86c8f037c8d42a"
      },
      "cell_type": "markdown",
      "source": "# 探索 "
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c"
      },
      "cell_type": "code",
      "source": "train.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e94f8371d8be87186f23ecc75178480d3d96bd78"
      },
      "cell_type": "code",
      "source": "train.loc[train.SentenceId == 2]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f75e24b86e3aeb7477fa1cd69789992233420b1"
      },
      "cell_type": "code",
      "source": "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e"
      },
      "cell_type": "code",
      "source": "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bef53d8f659b78e3fc6b1ed07025591d55b7c128"
      },
      "cell_type": "code",
      "source": "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e26b87ca71285ab9ee43a399d0534cbc03843df5"
      },
      "cell_type": "code",
      "source": "train['Sentiment'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2"
      },
      "cell_type": "markdown",
      "source": "`We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. **Sometimes one word or even one punctuation mark influences the sentiment**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d1efbed65f250d37472544f4fe37cb6fd13e183",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Let's see for example most common trigrams for positive phrases"
    },
    {
      "metadata": {
        "_uuid": "dea3ff8165291f82b0516dc7bf13691d653e2d31"
      },
      "cell_type": "markdown",
      "source": "ngrams:  几个为一组"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a128ab1d36eef6ec47161705a2b1094b830fe10"
      },
      "cell_type": "code",
      "source": "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext_trigrams = [i for i in ngrams(text.split(), 3)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "ccc69cbbe06084a848a35d47644d936334cbb479"
      },
      "cell_type": "code",
      "source": "text_trigrams[:3]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5c8533c5861794d44b53dbb2e5ef764e5e88119"
      },
      "cell_type": "code",
      "source": "Counter(text_trigrams).most_common(30)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "402c0b58fe43a680bea33e1813012bec5c16cb55"
      },
      "cell_type": "code",
      "source": "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext = [i for i in text.split() if i not in stopwords.words('english')]\ntext_trigrams = [i for i in ngrams(text, 3)]\nCounter(text_trigrams).most_common(30)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f59db6ed32fafb024728fd95b96157f278682a74"
      },
      "cell_type": "markdown",
      "source": "The results show the main problem with this dataset: **there are to many common words due to sentenced splitted in phrases**. As a result stopwords shouldn't be removed from text."
    },
    {
      "metadata": {
        "_uuid": "10dc8fc8d535ef492a3dab1b85b4052feec756ee"
      },
      "cell_type": "markdown",
      "source": "### Thoughts on feature processing and engineering"
    },
    {
      "metadata": {
        "_uuid": "d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b"
      },
      "cell_type": "markdown",
      "source": "So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n- puntuation could be important, so it should be used;\n- ngrams are necessary to get the most info from data;\n- using features like word count or sentence length won't be useful;"
    },
    {
      "metadata": {
        "_uuid": "72cdd2c86a3996ab7ca41971046dfff41d6a9d42"
      },
      "cell_type": "markdown",
      "source": "# 编码  tokenizer"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "tokenizer = TweetTokenizer()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "50c49caf376cd22a5af930c335813b3131e8bf4a"
      },
      "cell_type": "markdown",
      "source": "?? ngram_range"
    },
    {
      "metadata": {
        "_uuid": "828ed5a9ee5143b5542eccc6d90daa3e9ddc9edd"
      },
      "cell_type": "markdown",
      "source": "编码"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ebc937fd5ec811bc5c529ff416180fe338d073d"
      },
      "cell_type": "code",
      "source": "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d0bb4539b1e0b5f8439878a967ce7b5ece23f60"
      },
      "cell_type": "code",
      "source": "y = train['Sentiment']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "42d31643e53c1d2eb5f74d339bd494e5b49c3a25"
      },
      "cell_type": "code",
      "source": "train_vectorized[:2]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3a214237f75ef8ba2887d61f62c215459a3484f"
      },
      "cell_type": "code",
      "source": "help(OneVsRestClassifier)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb023b286a7f05f06c55c8b2bcbb46ccbe7c041b"
      },
      "cell_type": "markdown",
      "source": "# 训练 "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75a7acfd815fb391cad92eed8d2f13e5c9801de8"
      },
      "cell_type": "code",
      "source": "logreg = LogisticRegression()\novr = OneVsRestClassifier(logreg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a0bf05b880e0d05da378b753394e5a631753e82"
      },
      "cell_type": "code",
      "source": "%%time\novr.fit(train_vectorized, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "454e2a9316ba267ce66b1abf7da89da315351290"
      },
      "cell_type": "code",
      "source": "help(LinearSVC)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe"
      },
      "cell_type": "code",
      "source": "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "755c2b04f7bf8e86e9dc6242f392a575c61a052c"
      },
      "cell_type": "code",
      "source": "%%time\nsvc = LinearSVC(dual=False)\nscores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6ccaedd2a27eaa8d6859b2b66bd4d3ad3d0587e4"
      },
      "cell_type": "markdown",
      "source": "# 评估 "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55a5845de3412244cfa9f0a11392e50c76d78184"
      },
      "cell_type": "code",
      "source": "ovr.fit(train_vectorized, y);\nsvc.fit(train_vectorized, y);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1"
      },
      "cell_type": "markdown",
      "source": "## Deep learning\nAnd now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e7caffe8462d1c37b4e0b0bbe60190cde7231328"
      },
      "cell_type": "markdown",
      "source": "1.keras.layers:\n    1.  Bidirectional rnn: 双向rnn\n    2. GRU:  Gated Recurrent Unit, LSTM 变体, 加入忘记门和输入门 \n    3.  GlobalMaxPool1D, MaxPooling1D [差别](https://stackoverflow.com/questions/43728235/what-is-the-difference-between-keras-maxpooling1d-and-globalmaxpooling1d-functi)\n    4. constrains:  functions that impose constraints on weight values(e.g MaxNorm, MinMaxNorm)"
    },
    {
      "metadata": {
        "_uuid": "f8f66a8817fe7c3ec8b961551ee68a12e16d0f86"
      },
      "cell_type": "markdown",
      "source": "# 编码 tokennizer"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c9acc6a28970c178ad63ff347b7a118a7b7b13f"
      },
      "cell_type": "code",
      "source": "import keras\ndir(keras)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80"
      },
      "cell_type": "code",
      "source": "tk = Tokenizer(lower = True, filters='')\ntk.fit_on_texts(full_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1724669c7ca010d1bbf75e200211afdead768d1f"
      },
      "cell_type": "code",
      "source": "train_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2891fd68d0e7cfe49ad882595b431856f8eee8a7"
      },
      "cell_type": "code",
      "source": "print(len(train_tokenized))\nprint(len(train_tokenized[0]))\nprint(len(train_tokenized[20]))\n\n\n# print(test_tokenized.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "af02306e353ea2ef4a8b537f75f4ee8294118d18"
      },
      "cell_type": "markdown",
      "source": "# pad ?? "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b"
      },
      "cell_type": "code",
      "source": "max_len = 50\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fcb4482d43f171509a5854b09dd6a8590a8ed1e0"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "399d2b1ee867e3223cdd078e1a7c929597fcc734",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "X_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6b1f4574457545c8d268c8e52c044c154a053338"
      },
      "cell_type": "markdown",
      "source": "# Embedding "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c"
      },
      "cell_type": "code",
      "source": "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d74c2aa2f13e8544f045e5644d5bac70e248a8bd"
      },
      "cell_type": "code",
      "source": "embed_size = 300\nmax_features = 30000",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdb522c23b75331481145b789cf127b39d47eaa1"
      },
      "cell_type": "code",
      "source": "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "539088c6a01cc12046298b7371628f5cc03ef613"
      },
      "cell_type": "code",
      "source": "word_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63bfe54e55dc24a6734970da5ea7ab9b46bc8b0e"
      },
      "cell_type": "code",
      "source": "embedding_matrix.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "617dc05409aacfd8739765111f72606db434af91"
      },
      "cell_type": "markdown",
      "source": "# build model and train "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "51844fcec05cd42f6de6751e2342ac8936909114"
      },
      "cell_type": "code",
      "source": "help(CuDNNGRU)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "594273c2d887315d083a35a5ffd7c2dd40c2ebb6"
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\ndef build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = \"best_model.hdf5\"\n    # callback function; save the model after every epoch\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n    \n    inp = Input(shape = (max_len,))\n    # 直接load训练好的为embedding_matrix 没在这儿训练\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    # 丢掉整个dim, 普通的丢掉几个元素而已\n    # ? 下一步如何衔接dim?\n    x1 = SpatialDropout1D(spatial_dr)(x)\n    \n    # bidirection wrapper in rnn\n    # CuDNN... 英伟达出品,只能在GPU上跑\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    # global average pooling operation for temporal data\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    # \n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    writer = tf.summary.FileWriter('logs/', sess.graph)\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "95f52b1f6de4e939c8d21e3525503912282fbd47"
      },
      "cell_type": "markdown",
      "source": "An attempt at ensemble:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5eb586c98fb75c25cac099cd03d8233185fdc317"
      },
      "cell_type": "code",
      "source": "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b059392aad7d904adfb8ae151ad2004aa03da30d"
      },
      "cell_type": "code",
      "source": "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe"
      },
      "cell_type": "code",
      "source": "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    \n    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n    \n    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n    \n    \n    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n    \n    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bf90d6d4effebb3c5aa9b666b8e09c9c57d94d3"
      },
      "cell_type": "code",
      "source": "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf6d8d367c5adc30e00bbd77c1de70fd52960441"
      },
      "cell_type": "code",
      "source": "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b10113439be683bf19930750eaf96328e5b58d42"
      },
      "cell_type": "code",
      "source": "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8529014d1a239f308ac5f2552088ce2d8ebb8966"
      },
      "cell_type": "code",
      "source": "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\npred = pred1\npred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred2\npred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred3\npred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred4\npred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d58a5b52ea647dab51123ef89878c5355b3d2971"
      },
      "cell_type": "code",
      "source": "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\nsub['Sentiment'] = predictions\nsub.to_csv(\"blend.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}