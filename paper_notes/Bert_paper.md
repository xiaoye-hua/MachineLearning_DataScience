# [Bert:Pre-training of Deep Bidirectional Transformers for Language Understanding](link_to_paper)

_8th December 2019_

tl;dr: Bert can obtain state-of-art results in several NLP tasks.

#### Overall impression

#### Key ideas
- Bert: Bidirectional Encoder Representations from Transformers
- The major limitation is that standard language models are unidirectional
-

#### Technical details
- Two steps in Bert framework:
  - pre-training
  - fine-tuning

#### Notes
- There are two exsting strategies for applying pre-trained language representations to downstream tasks:
  - feature-based
  - fine-tuning
