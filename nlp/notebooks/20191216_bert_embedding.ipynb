{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "474f5527e266016f7ea6920698cc85b8f4583aec"
   },
   "source": [
    "# Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "4ee652abb6bc2f69a61d388a2339c5efb45adb1b"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "from bert_serving.client import BertClient\n",
    "import numpy as np\n",
    "\n",
    "from src.data.DataLoader import DataLoader\n",
    "# from src.model.model1 import build_model\n",
    "from src.model.model2 import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug = True\n",
    "# if debug:\n",
    "#     dataset = \"../data_model/aclImdb/debug\"\n",
    "# else:\n",
    "#     dataset = \"../data_model/aclImdb/\"\n",
    "model_input_dim = 768\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "verbose = 1\n",
    "# callbacks = [check_point, earlystopping],\n",
    "validation_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bc = BertClient()\n",
    "# sentences = [\"first do it\", \"then do it right\", \"then do it better\"]\n",
    "sentences = [\n",
    "    \"最大的优点也就是价钱比较实惠，另外有免费停车场如果住在古镇里面，白天是不允许把车开进去的。这个宾馆的服务员都说自己的餐厅做的当地小吃好吃，实在不敢苟同，份量少，味道也不地道，价格却不低，建议重视当地美食的朋友不要在宾馆的餐厅就餐，会对山西的小吃产生错误\", # 2\n",
    "    \"这个配置和价位真的很合适，完全够用，而且小黑的质量非常不错\", # 1\n",
    "    \"入住感想如下找不到5星级宾馆的感觉、总体水平最多只能算4星级。虽然房价比中州皇冠便宜100元左右、但所有硬件及软件远不及中州皇冠\" # 0\n",
    "]\n",
    "train_x = bc.encode(sentences)\n",
    "train_y = np.array(\n",
    "    [[0],\n",
    "    [1],\n",
    "    [0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape([train_x.shape[0], 1, train_x.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guohua/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/guohua/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv1d/conv1d/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,128], [1,3,128,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv1d/conv1d/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,128], [1,3,128,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3420efefb973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ml_learning_materail/ml_learning/nlp/src/model/model2.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_dim, lr, lr_d, units, spatial_dr, kernel_size1, kernel_size2, dense_units, dr, conv_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mx_gru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'he_uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gru\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# global average pooling operation for temporal data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mavg_pool1_gru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# In graph mode, failure to build the layer's graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# implies a user-side bug. We don't catch exceptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'causal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv1d\u001b[0;34m(self, input, filter, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;31m# pylint: enable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m   3480\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3482\u001b[0;31m         data_format=data_format)\n\u001b[0m\u001b[1;32m   3483\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspatial_start_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m   1027\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/card/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv1d/conv1d/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,128], [1,3,128,32]."
     ]
    }
   ],
   "source": [
    "model = build_model(input_dim=model_input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 1, 768)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,049,857\n",
      "Trainable params: 1,049,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 327ms/sample - loss: 0.8852 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 2.0000 - fn: 1.0000 - accuracy: 0.6667 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 22ms/sample - loss: 0.4353 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 2.0000 - fn: 1.0000 - accuracy: 0.6667 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 25ms/sample - loss: 0.1878 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000     \n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 20ms/sample - loss: 0.0920 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000     \n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 23ms/sample - loss: 0.0485 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000     \n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 22ms/sample - loss: 0.0300 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000     \n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 17ms/sample - loss: 0.0180 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 21ms/sample - loss: 0.0113 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 21ms/sample - loss: 0.0077 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000     \n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 21ms/sample - loss: 0.0054 - tp: 1.0000 - fp: 0.0000e+00 - tn: 2.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f39d4b3c8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=train_x, \n",
    "    y=train_y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b99f99486e6a32cb8f935a1f0980bce230d74896"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e93ac9418a5d5d984d93c7aa9296617d756b7ee2"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bb07a7e0f6bf6976b307e5c2f0ef96f069c5424"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5dcfb0ec2aabf09f60fadb036be8d9b8987185b0"
   },
   "outputs": [],
   "source": [
    "help(np.percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c018894738475dc8615e32f5a7d2d70fd4ef85ba"
   },
   "outputs": [],
   "source": [
    "def df_info(df):\n",
    "    print('='*20)\n",
    "    print('# of row in df: {}'.format(len(df)))\n",
    "    print('# of PhraseId: {0}'.format(df.groupby('PhraseId').size().sum()))\n",
    "    print('avg # of SentencId in every PhraseId :{0:.0f}'.format(df.groupby('SentenceId').size().mean()))\n",
    "#     print('avg # of word in each sentence is: {0:.0f}'.format(df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).mean()))\n",
    "    values = df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).values\n",
    "#     print(len(values))\n",
    "#     print(np.percentile(values,[0, 25, 50, 75, 100]))\n",
    "    print('stat # of word in each sentence is: {}'.format(\n",
    "        np.percentile(\n",
    "            values, \n",
    "            [0, 25, 50, 75, 100]\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41641e27b0051cd60426dd5f79e958bad80e566e"
   },
   "outputs": [],
   "source": [
    "df_info(train)\n",
    "df_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfce2f4a2df6c3c6bcce887567f498df04ff5477"
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "values = df.apply(lambda row: len(row['Phrase'].split(' ')), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3495f185555f84c7f5b25977111191108ba6df19"
   },
   "outputs": [],
   "source": [
    "np.percentile(values, [0, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bcaf71749144128f530ec05672a391c0a5f00f53"
   },
   "source": [
    "# 传统机器学习过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b50ccfb59404f1f5be1ca7f5ff63fe6bcdc80508"
   },
   "source": [
    "## Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81a4fd1fb0cf5eb6fd28af354f07419f9f594917",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aca9fcb9aeefb2e6956e5a1a7453e3ff96d0e8f"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=[1, 2], tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d11993fad0ab0e5403bb81ec8b482f237c6eef8"
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05dcf8702bcf03df83c363fda8b2159dce3f3ae8"
   },
   "outputs": [],
   "source": [
    "train_x = vectorizer.transform(train['Phrase'])\n",
    "test_x = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c132da657bf3436cd88f52380a5f6b290252cbed"
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cccfe51f6da4b91e40356f4a1ea0164054e9f658"
   },
   "outputs": [],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c8666e73700fc0d6a0eeb2eeb678bdd4f639b93"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2613815e03267aa9e82050dcf8c7be111ef67c2e"
   },
   "source": [
    "## Train & evalationa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a979d131f67f8e548bd3ec1e3cd9114ea5394af9"
   },
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e0c4f83c2006ea8b9d13429f80225866b7902e7"
   },
   "outputs": [],
   "source": [
    "log = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b6f29a17348310150b2fb7a5dad3ddbe91352e5"
   },
   "outputs": [],
   "source": [
    "score = cross_val_score(ovr, X=train_x, y = y, cv=3).mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a139747019f643cce2e2c41683b073707486383"
   },
   "outputs": [],
   "source": [
    "svm = LinearSVC(\n",
    "#     dual='dual'\n",
    ")\n",
    "score = cross_val_score(svm, X=train_x, y = y, cv=3).mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "675399c6c8e9fc9b8e6b2675735c0e91c395b889"
   },
   "source": [
    "# 深度学习 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a1d86c5f499d119b9e7fb48c279571c513e82d5"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, Flatten, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAvgPool1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6687ae42ff67d1f86d68e5e5ad0ca021fb550d9d"
   },
   "source": [
    "## Tokenlized and embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2c3b87fea131fd512acadbe7ad92ef36489e572"
   },
   "source": [
    "### Tokenlized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f03cae1ba92afea627671124f3d7243595259fe"
   },
   "outputs": [],
   "source": [
    "# tk = Tokenizer(lower=True, filtesr=' ')\n",
    "tk = Tokenizer(lower=True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3928c8e073f2c95171c7449c0d9fe26c5008f273"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9526d5a80fcf467834b2e02e26f1045d941ad276"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "250b863b1ce06900361e27cef189127ca3fb31d5"
   },
   "source": [
    "###  pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47b70c5d08bd3d5a48f4d6e29a78892b5fc081f4"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "train_X = pad_sequences(maxlen=max_len, sequences=train_tokenized)\n",
    "test_X = pad_sequences(maxlen=max_len, sequences=test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "635d3ffd76edd48a1859ec107b68c39e830ae40b"
   },
   "outputs": [],
   "source": [
    "# ?????\n",
    "\n",
    "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "embed_size = 300\n",
    "max_features = 30000\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73c3347597762ecbcacc374f837e224cfde4c213"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49ecf314ba65a1eb5daf1026bbd87258a8df6b5c"
   },
   "outputs": [],
   "source": [
    "np.shape(y_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52bbe028e76c15fc2ce7f9b56eae45c364e84534"
   },
   "source": [
    "## Model defination and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da5a3eccdb2e605c0747edc03cfe7d32090435a0"
   },
   "outputs": [],
   "source": [
    "def build_model1(units, spatial_dr=1, cov_size=32, kernel_size1=3, kernel_size2=2, dense_unit=128, dr=0.1, lr=0.001, lr_d=0.0):\n",
    "    file_path = 'model.hdf5'\n",
    "    check_point = ModelCheckpoint(\n",
    "        filepath=file_path,\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    earlystopping = EarlyStopping(\n",
    "#         monitor = 'binary_crossentropy'\n",
    "        monitor = 'val_loss',\n",
    "        patience = 3,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    \n",
    "    # Embedding layer \n",
    "#     ????????  shape ??\n",
    "#     inp = Input(shape=(embed_size, ))\n",
    "    inp = Input(shape=(max_len, ))\n",
    "    x = Embedding(\n",
    "        input_dim = 19479,\n",
    "        output_dim = embed_size, \n",
    "        weights = [embedding_matrix],\n",
    "        trainable = False\n",
    "    )(inp)\n",
    "    x = SpatialDropout1D(spatial_dr)(x)\n",
    "    \n",
    "    # return_sequences=True, 输出cell_state 和hiden_state\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences=True))(x)\n",
    "#     print(x_gru.shape)\n",
    "    x1 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size1,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'\n",
    "    )(x_gru)\n",
    "    maxpool_x1 = GlobalMaxPooling1D()(x1)\n",
    "    avgpool_x1 = GlobalAvgPool1D()(x1)\n",
    "    \n",
    "    x2 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size2,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'        \n",
    "    )(x_gru)\n",
    "    maxpool_x2 = GlobalMaxPooling1D()(x2)\n",
    "    avgpool_x2 = GlobalAvgPool1D()(x2)    \n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n",
    "    x3 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size1,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'\n",
    "    )(x_lstm)\n",
    "    maxpool_x3 = GlobalMaxPooling1D()(x3)\n",
    "    avgpool_x3 = GlobalAvgPool1D()(x3)   \n",
    "    \n",
    "    x4 = Conv1D(\n",
    "        filters = cov_size,\n",
    "        kernel_size=kernel_size2,\n",
    "        padding = 'valid',\n",
    "        kernel_initializer = 'he_uniform'\n",
    "    )(x_lstm)\n",
    "    maxpool_x4 = GlobalMaxPooling1D()(x4)\n",
    "    avgpool_x4 = GlobalAvgPool1D()(x4)\n",
    "    \n",
    "    x = concatenate([\n",
    "                     maxpool_x1, avgpool_x1, \n",
    "                     maxpool_x2, avgpool_x2,\n",
    "                     maxpool_x3, avgpool_x3,\n",
    "                     maxpool_x4, avgpool_x4\n",
    "                    ])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_unit, activation='relu')(x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_unit/2), activation='relu')(x))\n",
    "#     output = Dense(5, activation='softmax')(x)\n",
    "    output = Dense(5, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(input=inp, output=output)\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=lr, decay=lr_d),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    model.fit(\n",
    "#         x = embedding_matrix,\n",
    "        x = train_X,\n",
    "#         y = y,\n",
    "        y= y_ohe,\n",
    "        batch_size = 128,\n",
    "        epochs = 20,\n",
    "        verbose = 1,\n",
    "        callbacks = [check_point, earlystopping],\n",
    "        validation_split = 0.1\n",
    "    )\n",
    "    model.load_model(file_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "070d3ec511ff38e611c394992d794efc1951eeb9"
   },
   "outputs": [],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_unit=32, dr=0.1, cov_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d23e47517cb6e10c1a35e6e198d817e7a7ef6702"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1"
   },
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n",
    "\n",
    "This dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dea95068fd46eef3b78ee878c86c8f037c8d42a"
   },
   "source": [
    "# 探索 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c"
   },
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e94f8371d8be87186f23ecc75178480d3d96bd78"
   },
   "outputs": [],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f75e24b86e3aeb7477fa1cd69789992233420b1"
   },
   "outputs": [],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e"
   },
   "outputs": [],
   "source": [
    "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
    "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bef53d8f659b78e3fc6b1ed07025591d55b7c128"
   },
   "outputs": [],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e26b87ca71285ab9ee43a399d0534cbc03843df5"
   },
   "outputs": [],
   "source": [
    "train['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2"
   },
   "source": [
    "`We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. **Sometimes one word or even one punctuation mark influences the sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d1efbed65f250d37472544f4fe37cb6fd13e183",
    "collapsed": true
   },
   "source": [
    "Let's see for example most common trigrams for positive phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dea3ff8165291f82b0516dc7bf13691d653e2d31"
   },
   "source": [
    "ngrams:  几个为一组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a128ab1d36eef6ec47161705a2b1094b830fe10"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccc69cbbe06084a848a35d47644d936334cbb479",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_trigrams[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5c8533c5861794d44b53dbb2e5ef764e5e88119"
   },
   "outputs": [],
   "source": [
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "402c0b58fe43a680bea33e1813012bec5c16cb55"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f59db6ed32fafb024728fd95b96157f278682a74"
   },
   "source": [
    "The results show the main problem with this dataset: **there are to many common words due to sentenced splitted in phrases**. As a result stopwords shouldn't be removed from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10dc8fc8d535ef492a3dab1b85b4052feec756ee"
   },
   "source": [
    "### Thoughts on feature processing and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b"
   },
   "source": [
    "So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n",
    "- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n",
    "- puntuation could be important, so it should be used;\n",
    "- ngrams are necessary to get the most info from data;\n",
    "- using features like word count or sentence length won't be useful;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72cdd2c86a3996ab7ca41971046dfff41d6a9d42"
   },
   "source": [
    "# 编码  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50c49caf376cd22a5af930c335813b3131e8bf4a"
   },
   "source": [
    "?? ngram_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "828ed5a9ee5143b5542eccc6d90daa3e9ddc9edd"
   },
   "source": [
    "编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ebc937fd5ec811bc5c529ff416180fe338d073d"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d0bb4539b1e0b5f8439878a967ce7b5ece23f60"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42d31643e53c1d2eb5f74d339bd494e5b49c3a25",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vectorized[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3a214237f75ef8ba2887d61f62c215459a3484f"
   },
   "outputs": [],
   "source": [
    "help(OneVsRestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb023b286a7f05f06c55c8b2bcbb46ccbe7c041b"
   },
   "source": [
    "# 训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75a7acfd815fb391cad92eed8d2f13e5c9801de8"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a0bf05b880e0d05da378b753394e5a631753e82"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "454e2a9316ba267ce66b1abf7da89da315351290"
   },
   "outputs": [],
   "source": [
    "help(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe"
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "755c2b04f7bf8e86e9dc6242f392a575c61a052c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ccaedd2a27eaa8d6859b2b66bd4d3ad3d0587e4"
   },
   "source": [
    "# 评估 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55a5845de3412244cfa9f0a11392e50c76d78184"
   },
   "outputs": [],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7caffe8462d1c37b4e0b0bbe60190cde7231328"
   },
   "source": [
    "1.keras.layers:\n",
    "    1.  Bidirectional rnn: 双向rnn\n",
    "    2. GRU:  Gated Recurrent Unit, LSTM 变体, 加入忘记门和输入门 \n",
    "    3.  GlobalMaxPool1D, MaxPooling1D [差别](https://stackoverflow.com/questions/43728235/what-is-the-difference-between-keras-maxpooling1d-and-globalmaxpooling1d-functi)\n",
    "    4. constrains:  functions that impose constraints on weight values(e.g MaxNorm, MinMaxNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8f66a8817fe7c3ec8b961551ee68a12e16d0f86"
   },
   "source": [
    "# 编码 tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c9acc6a28970c178ad63ff347b7a118a7b7b13f"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "dir(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80"
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1724669c7ca010d1bbf75e200211afdead768d1f"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2891fd68d0e7cfe49ad882595b431856f8eee8a7"
   },
   "outputs": [],
   "source": [
    "print(len(train_tokenized))\n",
    "print(len(train_tokenized[0]))\n",
    "print(len(train_tokenized[20]))\n",
    "\n",
    "\n",
    "# print(test_tokenized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af02306e353ea2ef4a8b537f75f4ee8294118d18"
   },
   "source": [
    "# pad ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcb4482d43f171509a5854b09dd6a8590a8ed1e0"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "399d2b1ee867e3223cdd078e1a7c929597fcc734",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b1f4574457545c8d268c8e52c044c154a053338"
   },
   "source": [
    "# Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d74c2aa2f13e8544f045e5644d5bac70e248a8bd"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdb522c23b75331481145b789cf127b39d47eaa1"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "539088c6a01cc12046298b7371628f5cc03ef613"
   },
   "outputs": [],
   "source": [
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63bfe54e55dc24a6734970da5ea7ab9b46bc8b0e"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "617dc05409aacfd8739765111f72606db434af91"
   },
   "source": [
    "# build model and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51844fcec05cd42f6de6751e2342ac8936909114",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(CuDNNGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "594273c2d887315d083a35a5ffd7c2dd40c2ebb6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    # callback function; save the model after every epoch\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    # 直接load训练好的为embedding_matrix 没在这儿训练\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    # 丢掉整个dim, 普通的丢掉几个元素而已\n",
    "    # ? 下一步如何衔接dim?\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "    \n",
    "    # bidirection wrapper in rnn\n",
    "    # CuDNN... 英伟达出品,只能在GPU上跑\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    # global average pooling operation for temporal data\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    # \n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f52b1f6de4e939c8d21e3525503912282fbd47"
   },
   "source": [
    "An attempt at ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5eb586c98fb75c25cac099cd03d8233185fdc317"
   },
   "outputs": [],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b059392aad7d904adfb8ae151ad2004aa03da30d"
   },
   "outputs": [],
   "source": [
    "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe"
   },
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bf90d6d4effebb3c5aa9b666b8e09c9c57d94d3"
   },
   "outputs": [],
   "source": [
    "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf6d8d367c5adc30e00bbd77c1de70fd52960441"
   },
   "outputs": [],
   "source": [
    "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b10113439be683bf19930750eaf96328e5b58d42"
   },
   "outputs": [],
   "source": [
    "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8529014d1a239f308ac5f2552088ce2d8ebb8966"
   },
   "outputs": [],
   "source": [
    "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred1\n",
    "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred2\n",
    "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred3\n",
    "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d58a5b52ea647dab51123ef89878c5355b3d2971"
   },
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['Sentiment'] = predictions\n",
    "sub.to_csv(\"blend.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(card)",
   "language": "python",
   "name": "card"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
